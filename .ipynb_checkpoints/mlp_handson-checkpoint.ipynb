{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/I-tomotsuka/Frontier-Artificial-Intelligence/blob/master/mlp_handson.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "13Zv9_PqQdSI"
   },
   "source": [
    "# Multi Layer Perceptron (MLP) の実装"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jqkJ33OpQdSJ"
   },
   "source": [
    "# 目次\n",
    "\n",
    "1. [全体像](#pipeline)\n",
    "1. [目標](#goal)\n",
    "- [下準備](#prepare)\n",
    "- [活性化関数](#activate)\n",
    "- [線形層](#linear)\n",
    "- [多層パーセプトロン](#mlp)\n",
    "- [学習](#train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1i-R_K4aQdSL"
   },
   "source": [
    "# 1. 全体像<a class=\"anchor\" id=\"pipeline\"></a>\n",
    "- 変数のあとの括弧は，行列の形状 (numpy.ndarrayのshape) を表す．\n",
    "- Nはバッチサイズ\n",
    "\n",
    "※ バッチ処理とは……画像を1枚ずつ処理するのではなく，複数枚まとめて処理すること．画像1枚は，28×28のサイズから(784, )の形状をもつ1次元配列になるようにリサイズされているが，これをN枚まとめて(N, 784)の2次元配列として扱う．N枚まとめて行列演算を実行することで，1枚ずつ処理するより高速に処理できるようになる．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Ym-PX-LSHzKQ"
   },
   "source": [
    "<img src=\"https://drive.google.com/uc?export=download&id=1q-o-iqPQ5gdDajty-sFNEJKYWRRXicW5\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "j4prYVN-QdSN"
   },
   "source": [
    "## 2. 目標<a class=\"anchor\" id=\"goal\"></a>\n",
    "- 多層 (例では3層) のニューラルネットワークを構築する．\n",
    "- まず**活性化関数**をそれぞれ実装し，次に**線形層**の実装を行い，最後にそれらをまとめて**多層パーセプトロン**の実装を行う．\n",
    "- 最終的には，下のように各レイヤーの入出力のユニット数と活性化関数を指定するだけでモデルが構築できるようにする．\n",
    "- 例：3層・隠れ層のユニット数が1000．活性化関数はReLUを用いる場合\n",
    "```python\n",
    "model = MLP([Linear(784, 1000, ReLU),\n",
    "                        Linear(1000, 1000, ReLU),\n",
    "                        Linear(1000, 10, Softmax)])\n",
    "```\n",
    "- こうすると，4層以上への拡張や，活性化関数の変更などがしやすい"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0JS2J7NlQdSO"
   },
   "source": [
    "## 3. 下準備<a class=\"anchor\" id=\"prepare\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cHmydwnpQdSO"
   },
   "source": [
    "### ライブラリのインポート"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XII_cLmkQdSP"
   },
   "source": [
    "- matplotlib: 図やグラフの描画など．\n",
    "- numpy: 行列演算など\n",
    "- sklearn: scikit-learn．様々な機械学習のモデルが利用できるが，今回はMNISTのデータをダウンロードするのに用いる．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9YnjWwdAHzKU"
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.model_selection import train_test_split\n",
    "#from __future__ import unicode_literals\n",
    "#from __future__ import print_function\n",
    "#from __future__ import division\n",
    "#from __future__ import absolute_import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "id": "naAKnEAfQ80X",
    "outputId": "22c2de35-673c-492c-be7a-a656376a4b0c"
   },
   "outputs": [],
   "source": [
    "#from google.colab import drive # driveを接続\n",
    "#drive.mount('/content/gdrive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "ERDFaEjvINmp",
    "outputId": "f795d03f-85ed-4966-924e-03a23f2efded"
   },
   "outputs": [],
   "source": [
    "# drive中の課題ファイルのあるディレクトリに移動\n",
    "#%cd /content/gdrive/My Drive/sentanjinnkouchinou/先端人工知能論I_200512\n",
    "\n",
    "#from test_mlp import *  # テスト用"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-VNIETHcQdST"
   },
   "source": [
    "### MNISTデータの読み込み"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2FGDjTIMQdSU"
   },
   "source": [
    "- データをダウンロードする．一度ダウンロードすると，その後はデータを参照して読み込んでくれるので，毎回ダウンロードしなくても良くなる．\n",
    "- Xが画像データ，Yが正解データ\n",
    "- mnistのデータは，0~255のint型で表されているが，これを**255で割って正規化**する．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6BZJ3zTDQdSV"
   },
   "outputs": [],
   "source": [
    "X, Y = fetch_openml('mnist_784', version=1, data_home=\"./data/\", return_X_y=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pHscB6lSQdSX"
   },
   "outputs": [],
   "source": [
    "#X = X / 255.\n",
    "X_average = X.mean(axis = 1)\n",
    "X_stddev = X.std(axis = 1)\n",
    "X_numofsample = X.shape[0]\n",
    "X = (X - X_average.reshape(X_numofsample, 1))/X_stddev.reshape(X_numofsample, 1)\n",
    "Y = Y.astype(\"int\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bwuJYGVDQdSZ"
   },
   "source": [
    "### データセットの可視化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 207
    },
    "colab_type": "code",
    "id": "gEf1SMSNQdSa",
    "outputId": "aa323f93-8265-44b6-e884-d627ed2c1d8b"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 10 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    plt.subplot(2, 5, i + 1)\n",
    "    plt.imshow(X[i * 6500].reshape(28, 28), cmap='gray_r')\n",
    "    plt.axis(\"off\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SoeLXRkBQdSd"
   },
   "source": [
    "### 訓練・テストデータに分割"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lyT_cKgnQdSe"
   },
   "source": [
    "- 訓練データで学習し，同じ訓練データで性能の評価を行うと，訓練データでは良い性能を示すが，データを少しでも変えると性能が低下してしまうことがある（**過学習**）．\n",
    "- <span style=\"text-decoration: underline\">よって，学習する訓練データとは異なるテストデータで性能評価を行う</span>．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2TqTX6ViQdSf"
   },
   "outputs": [],
   "source": [
    "train_x, test_x, train_y, test_y = train_test_split(X, Y, test_size=0.2, random_state=2)\n",
    "train_y = np.eye(10)[train_y].astype(np.int32)\n",
    "test_y = np.eye(10)[test_y].astype(np.int32)\n",
    "train_n = train_x.shape[0]\n",
    "test_n = test_x.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BB3F7Kt8QdSo"
   },
   "source": [
    "## 4. 活性化関数の実装<a class=\"anchor\" id=\"activate\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tGt8wTodQdSq"
   },
   "source": [
    "- ここでは，活性化関数として広く知られる Sigmoid関数と，ReLU関数，及び出力層の活性化関数であるSoftmax関数の実装を行う．\n",
    "- 各関数の詳細については，講義スライドを参照"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ol4lEIZuQdSr"
   },
   "source": [
    "### <font color=\"crimson\">課題</font>：Sigmoid関数の実装"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ABnSViAHQdSt"
   },
   "source": [
    "- 順伝播計算\n",
    "\n",
    "    - $h(x) = \\sigma(x) = \\dfrac{1}{1+e^{-x}}$\n",
    "\n",
    "- 逆伝播計算\n",
    "\n",
    "    - $h'(x) = \\sigma'(x) = \\sigma(x)\\;(1-\\sigma(x))$\n",
    "\n",
    "※ 順伝播の $\\sigma(x)$ と，逆伝播の $\\sigma(x)$ は同じなので，2回計算しなくてもOK"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ezjZetRAQdSu"
   },
   "source": [
    "<details>\n",
    "    <summary>ヒント</summary>\n",
    "    <div>\n",
    "        <br>\n",
    "        - np.exp(x) を用いて$\\exp(x)$を計算できる\n",
    "        <br>\n",
    "        - 順伝播の計算結果は， self.y に保存されているので，逆伝播計算ではそれを使おう\n",
    "    </div>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LQgU4_SBQdSv"
   },
   "outputs": [],
   "source": [
    "class Sigmoid:\n",
    "    def __init__(self):\n",
    "        self.y = None\n",
    "        \n",
    "    def __call__(self, x):\n",
    "        y = 1/(1+np.exp(-x))\n",
    "        self.y = y\n",
    "        return y\n",
    "    \n",
    "    def backward(self):\n",
    "        ry = self.y*(1 - self.y)\n",
    "        return ry  # 逆伝播計算"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "zYUIf9IXQdSz",
    "outputId": "c95c5375-954e-4a13-e2c7-54f1b197d0b6"
   },
   "outputs": [],
   "source": [
    "#test_sigmoid(Sigmoid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1aXj3-i6QdS3"
   },
   "source": [
    "### ReLU関数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8J0o5YWSQdS4"
   },
   "source": [
    "- 順伝播\n",
    "    - $h(x) = \\max(0, x)$\n",
    "- 逆伝播\n",
    "    - $h'(x) =\n",
    "        \\left\\{ \\begin{array}{ll}\n",
    "            1 & (x > 0) \\\\\n",
    "            0 & (x \\leq 0) \\\\\n",
    "        \\end{array} \\right.$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8T-6aCEJQdS6"
   },
   "source": [
    "<details>\n",
    "    <summary>ヒント</summary>\n",
    "    <div>\n",
    "        - ndarrayから，0より大きい要素をTrue, 0以下の要素をFalseとなるようなマスクを作成する\n",
    "        <div>\n",
    "        <pre style=\"background-color: whitesmoke;\"><code style=\"background-color: whitesmoke;\">\n",
    "        >>> a = np.array([[-1, 1], [0, 1]])\n",
    "        >>> print(a)\n",
    "        [[-1  1]\n",
    "         [ 0  1]]\n",
    "        >>> print(a > 0)\n",
    "        [[False  True]\n",
    "         [False  True]]\n",
    "        >>> print(a * (a>0))\n",
    "        [[0 1]\n",
    "         [0 1]]\n",
    "         </code></pre>\n",
    "        </div>\n",
    "    </div>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "U6_wTFdiQdS7"
   },
   "outputs": [],
   "source": [
    "class ReLU:\n",
    "    def __init__(self):\n",
    "        self.x = None\n",
    "        \n",
    "    def __call__(self, x):\n",
    "        self.x = x\n",
    "        return x * (x>0)  # 順伝播計算\n",
    "    \n",
    "    def backward(self):\n",
    "        return 1 * (self.x>0)  # 逆伝播計算"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "LnZ5pNqoQdS9",
    "outputId": "bd0092bb-b580-4d04-c079-5f7222152dc2"
   },
   "outputs": [],
   "source": [
    "#test_relu(ReLU)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "991-7gU-QdS_"
   },
   "source": [
    "### Softmax関数\n",
    "\n",
    "- ロジスティック回帰と同様に実装する\n",
    "\n",
    "- 入力：$\\boldsymbol{X}=(\\boldsymbol{x_1},\\boldsymbol{x_2},\\cdots,\\boldsymbol{x_N})^T\\in\\mathbb{R}^{N\\times K}$（データ行列）\n",
    "\n",
    "\n",
    "- 出力：$\\boldsymbol{Y}=(\\boldsymbol{y_1},\\boldsymbol{y_2},\\cdots,\\boldsymbol{y_N})^T\\in\\mathbb{R}^{N\\times K},\\,\\,\\,y_{nk} = softmax(\\boldsymbol{x_n})_k$\n",
    "\n",
    "\n",
    "- オーバーフローを防ぐために$\\boldsymbol{x}_n$の最大値を$\\boldsymbol{x}_n$自身から引く\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "softmax(\\boldsymbol{x})_k&= \\frac{\\exp (x_{k})} {\\Sigma_{i=1}^{K}{\\exp (x_{i})}}=\\frac{\\exp (-x_{max})\\exp (x_{k})}{\\exp (-x_{max})\\Sigma_{i=1}^{K}{\\exp (x_{i})}}=\\frac{\\exp (x_{k}-x_{max})} {\\Sigma_{i=1}^{K}{\\exp (x_{i}-x_{max})}}\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Kd0fV0JpQdTA"
   },
   "source": [
    "<details>\n",
    "    <summary>ヒント: 最大値の取得</summary>\n",
    "    <div>\n",
    "        - ndarrayから，最大値を取得したい\n",
    "        <pre style=\"background-color: whitesmoke;\"><code style=\"background-color: whitesmoke;\">\n",
    "        >>> A = np.array([[1, 2], [3, 4], [5, 6]])\n",
    "        >>> print(A.shape)\n",
    "        (3, 2)\n",
    "        >>> A.max()\n",
    "        6\n",
    "        </code></pre>\n",
    "        - ここでは，N×Kの配列について，1次元目の要素ごとに計N個の最大値を取得したい\n",
    "        <pre style=\"background-color: whitesmoke;\"><code style=\"background-color: whitesmoke;\">\n",
    "        >>> A.max(axis=1)\n",
    "        array([2, 4, 6])\n",
    "        </code></pre>\n",
    "        - さらに，配列の形状はN×1にしておきたい\n",
    "        <pre style=\"background-color: whitesmoke;\"><code style=\"background-color: whitesmoke;\">\n",
    "        >>> A.max(axis=1, keepdims=True)\n",
    "        array([[2],\n",
    "       [4],\n",
    "       [6]])\n",
    "        </code></pre>\n",
    "    </div>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Z8TQ9BpQQdTB"
   },
   "outputs": [],
   "source": [
    "class Softmax:\n",
    "    def __init__(self):\n",
    "        self.y = None\n",
    "        \n",
    "    def __call__(self, x):\n",
    "      exp_x = np.exp(x - x.max(axis=1, keepdims = True))\n",
    "      y = exp_x / np.sum(exp_x, axis=1, keepdims = True)\n",
    "      self.y = y\n",
    "      return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "wdd4HC4XQdTE",
    "outputId": "6dccce26-44c8-4ea5-e9a5-dff9202e1d99"
   },
   "outputs": [],
   "source": [
    "#test_softmax(Softmax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yBOzglnaJQJC"
   },
   "source": [
    "### Swish関数（実装途中）\n",
    "\n",
    "\n",
    "- 入力：$\\boldsymbol{X}=(\\boldsymbol{x_1},\\boldsymbol{x_2},\\cdots,\\boldsymbol{x_N})^T\\in\\mathbb{R}^{N\\times K}$（データ行列）\n",
    "\n",
    "\n",
    "- 出力：$\\boldsymbol{Y}=(\\boldsymbol{y_1},\\boldsymbol{y_2},\\cdots,\\boldsymbol{y_N})^T\\in\\mathbb{R}^{N\\times K},\\,\\,\\,y_{nk} = swish(\\boldsymbol{x_n})_k$\n",
    "\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "swish(\\boldsymbol{x})_k&=\\sigma(\\boldsymbol{x})x\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "myq9WcgBLU6t"
   },
   "outputs": [],
   "source": [
    "class swish:\n",
    "    def __init__(self):\n",
    "        self.y = None\n",
    "        \n",
    "    def __call__(self, x, beta):\n",
    "      y = x*(1/(1+np.exp(-beta*x)))\n",
    "      return y\n",
    "    \n",
    "    def backward(self):\n",
    "      return   # 逆伝播計算"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xWu_6hpciv60"
   },
   "source": [
    "### Mish関数（導関数は手計算）\n",
    "https://arxiv.org/abs/1908.08681\n",
    "\n",
    "Mish: A Self Regularized Non-Monotonic Neural Activation Function\n",
    "\n",
    "\n",
    "- 入力：$\\boldsymbol{X}=(\\boldsymbol{x_1},\\boldsymbol{x_2},\\cdots,\\boldsymbol{x_N})^T\\in\\mathbb{R}^{N\\times K}$（データ行列）\n",
    "\n",
    "\n",
    "- 出力：$\\boldsymbol{Y}=(\\boldsymbol{y_1},\\boldsymbol{y_2},\\cdots,\\boldsymbol{y_N})^T\\in\\mathbb{R}^{N\\times K},\\,\\,\\,y_{nk} = mish(\\boldsymbol{x_n})_k$\n",
    "\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "mish(x)&=x\\cdot \\tanh(softplus(\\boldsymbol{x}))\n",
    "\\end{align}\n",
    "$$\n",
    "$$\n",
    "\\begin{align}\n",
    "mish'(x)&=\\dfrac{e^{x}(4(x+1)+4e^{2x}+e^{3x}+e^{4x}(4x+6))}{(2e^{x}+e^{2x}+2)^2}\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hGlH9YUzitBJ"
   },
   "outputs": [],
   "source": [
    "def tanh(x):\n",
    "  return (np.exp(x) - np.exp(-x)) / (np.exp(x) + np.exp(-x))\n",
    "def softplus(x):\n",
    "  return np.log(1.0 + np.exp(x))\n",
    "def omega(x):\n",
    "  return 4*(x+1) + 4*np.exp(2*x) + np.exp(3*x) + np.exp(x)*(4*x+6)\n",
    "def delta(x):\n",
    "  return 2*np.exp(x) + np.exp(2*x) + 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0fk9ivlTlD_C"
   },
   "outputs": [],
   "source": [
    "class mish:\n",
    "    def __init__(self):\n",
    "        self.x = None\n",
    "      \n",
    "    def __call__(self, x):\n",
    "        self.x = x \n",
    "        y = x * tanh(softplus(x))\n",
    "        return y  # 順伝播計算\n",
    "    \n",
    "    def backward(self):\n",
    "        y = np.exp(self.x) * omega(self.x) / delta(self.x)**2\n",
    "        return y  # 逆伝播計算"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Aheob1svHzLO"
   },
   "source": [
    "## 5. 線形層<a class=\"anchor\" id=\"linear\"></a>\n",
    "- 引数\n",
    "    - in_dim : 入力のユニット数\n",
    "    - out_dim : 出力のユニット数\n",
    "    - activation : 活性化関数\n",
    "- 計算\n",
    "    - 順伝播\n",
    "        <div class=\"clearfix\">\n",
    "        <img style=\"float: left;\" src=\"https://drive.google.com/uc?export=download&id=1cpmyxw8vSzLjwmMXgpWVTJrjrW3Nqzhh\" width=200px>\n",
    "\n",
    "        </div>\n",
    "        - ヒント : np.dot(A, B)\n",
    "    - 逆伝播\n",
    "        - 入力 (dout)\n",
    "            - 一つ上の層 (l+1層) からの出力 $(\\boldsymbol{o}^{(l+1)})$\n",
    "        - 誤差\n",
    "        <div class=\"clearfix\">\n",
    "        <img style=\"float: left;\" src=\"https://drive.google.com/uc?export=download&id=13fq5TC9KlwZ08qrpu5aYxPCL_XAT4AsZ\" width=200px>\n",
    "        </div>\n",
    "            - $\\odot$ は要素積\n",
    "        - 勾配計算\n",
    "        \n",
    "        <div class=\"clearfix\">\n",
    "        <img style=\"float: left;\" src=\"https://drive.google.com/uc?export=download&id=1_1FtLMvBMMgLJwKbuuRPWd_RLCFpo2p5\" width=200px>\n",
    "        </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2TbjOmbYQdTH"
   },
   "source": [
    "<details>\n",
    "    <summary>ヒント: 行列積の計算について</summary>\n",
    "    <br>\n",
    "    <div>\n",
    "        $\n",
    "A = \\left(\n",
    "    \\begin{array}{ccc}\n",
    "      0 & 1 & 2 \\\\\n",
    "      1 & 2 & 3\n",
    "    \\end{array}\n",
    "  \\right)\n",
    "$ , \n",
    "$\n",
    "B = \\left(\n",
    "\\begin{array}{cc}\n",
    "  0 & 1\\\\\n",
    "  1 & 2 \\\\\n",
    "  2 & 3\n",
    "\\end{array}\n",
    "\\right)\n",
    "$ としたとき， $C = AB$ を計算する例\n",
    "        <br>\n",
    "        <div>\n",
    "        <div>\n",
    "        <pre style=\"background-color: whitesmoke;\"><code style=\"background-color: whitesmoke;\">\n",
    "        >>> A = np.array([[0, 1, 2], [1, 2, 3]])\n",
    "        >>> B = np.array([[0, 1], [1, 2], [2, 3]])\n",
    "        >>> print(A.shape, B.shape)\n",
    "        (2, 3) (3, 2)\n",
    "        >>> print(a > 0)\n",
    "        [[False  True]\n",
    "         [False  True]]\n",
    "        >>> C = np.dot(A, B)\n",
    "        >>> print(C)\n",
    "        [[ 5  8]\n",
    "         [ 8 14]]\n",
    "         </code></pre>\n",
    "          </div>\n",
    "        </div>\n",
    "    </div>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8-1axFyoQdTI"
   },
   "source": [
    "<details>\n",
    "    <summary>ヒント: 行列の要素積</summary>\n",
    "    <br>\n",
    "    <div>\n",
    "        <div>\n",
    "        <pre style=\"background-color: whitesmoke;\"><code style=\"background-color: whitesmoke;\">\n",
    "        >>> A = np.array([[0, 1], [2, 3]])\n",
    "        >>> B = np.array([[1, 2], [3, 4]])\n",
    "        >>> print(A*B)\n",
    "        [[ 0  2]\n",
    "         [ 6 12]]\n",
    "         </code></pre>\n",
    "        </div>\n",
    "    </div>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "a0d8WJG-QdTJ"
   },
   "outputs": [],
   "source": [
    "class Linear:\n",
    "    def __init__(self, in_dim, out_dim, activation):\n",
    "        self.W = np.random.uniform(low=-0.08, high=0.08, size=(in_dim, out_dim)) #ただの一様分布\n",
    "        #self.W = np.random.uniform() Xavierの一様分布\n",
    "        #self.W = np.random.normal(0, np.sqrt(2/in_dim), size=(in_dim, out_dim)) # Heの正規分布，　ReLU用\n",
    "        #self.W = np.random.uniform(low = -np.sqrt(in_dim), high = np.sqrt(in_dim), size=(in_dim, out_dim)) # Heの一様分布，　ReLU用\n",
    "        self.b = np.zeros(out_dim)\n",
    "        self.activation = activation()\n",
    "        self.delta = None\n",
    "        self.x = None\n",
    "        self.dW = None\n",
    "        self.db = None\n",
    "\n",
    "    def __call__(self, x, train_flg):\n",
    "        # 順伝播計算\n",
    "        self.x = x\n",
    "        u = np.dot(x, self.W) + self.b  # self.W, self.b, x を用いて u を計算しよう\n",
    "        self.z = self.activation(u)\n",
    "        return self.z\n",
    "    \n",
    "    def backward(self, dout):\n",
    "        # 誤差計算\n",
    "        self.delta = dout*self.activation.backward() # dout と活性化関数の逆伝播 (self.activation.backward()) を用いて delta を計算しよう\n",
    "        dout = np.dot(self.delta, self.W.T) # self.delta, self.W を用いて 出力 o を計算しよう\n",
    "        \n",
    "        # 勾配計算\n",
    "        self.dW = np.dot(self.x.T,self.delta)  # dW を計算しよう\n",
    "        self.db = np.dot(np.ones(len(self.x)),self.delta)# db を計算しよう\n",
    "        \n",
    "        return dout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "X7hzZ3xjHE8z"
   },
   "outputs": [],
   "source": [
    "class Dropout:\n",
    "    def __init__(self, in_dim, out_dim, dropout_ratio=0.5):\n",
    "        self.dropout_ratio = dropout_ratio\n",
    "        self.mask = None\n",
    "        self.W = np.random.uniform(low=-0.08, high=0.08, size=(in_dim, out_dim))\n",
    "        self.b = np.zeros(out_dim)\n",
    "        self.delta = 0\n",
    "        self.x = 0\n",
    "        self.dW = 0\n",
    "        self.db = 0\n",
    "\n",
    "    def __call__(self, x, train_flg):\n",
    "      if train_flg:\n",
    "        self.mask = np.random.rand(*x.shape) > self.dropout_ratio\n",
    "        return x * self.mask\n",
    "      else:\n",
    "        return x * (1.0 - self.dropout_ratio)\n",
    "    def backward(self, dout):\n",
    "        return dout * self.mask\n",
    "# dropoutよくわからん"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MD8we7rPG7f6"
   },
   "outputs": [],
   "source": [
    "class BatchNormalization:\n",
    "    def __init__(self, gamma, beta, momentum=0.9, running_mean=None, running_var=None):\n",
    "        self.gamma = gamma\n",
    "        self.beta = beta\n",
    "        self.momentum = momentum\n",
    "        self.input_shape = None # Conv層の場合は4次元、全結合層の場合は2次元  \n",
    "        self.running_mean = running_mean\n",
    "        self.running_var = running_var  \n",
    "        \n",
    "        # backward時に使用する中間データ\n",
    "        self.batch_size = None\n",
    "        self.xc = None\n",
    "        self.std = None\n",
    "        self.dgamma = None\n",
    "        self.dbeta = None\n",
    "\n",
    "    def forward(self, x, train_flg=True):\n",
    "        self.input_shape = x.shape\n",
    "        if x.ndim != 2:\n",
    "            N, C, H, W = x.shape\n",
    "            x = x.reshape(N, -1)\n",
    "\n",
    "        out = self.__forward(x, train_flg)\n",
    "        \n",
    "        return out.reshape(*self.input_shape)\n",
    "            \n",
    "    def __forward(self, x, train_flg):\n",
    "        if self.running_mean is None:\n",
    "            N, D = x.shape\n",
    "            self.running_mean = np.zeros(D)\n",
    "            self.running_var = np.zeros(D)\n",
    "                        \n",
    "        if train_flg:\n",
    "            mu = x.mean(axis=0)\n",
    "            xc = x - mu\n",
    "            var = np.mean(xc**2, axis=0)\n",
    "            std = np.sqrt(var + 10e-7)\n",
    "            xn = xc / std\n",
    "            \n",
    "            self.batch_size = x.shape[0]\n",
    "            self.xc = xc\n",
    "            self.xn = xn\n",
    "            self.std = std\n",
    "            self.running_mean = self.momentum * self.running_mean + (1-self.momentum) * mu\n",
    "            self.running_var = self.momentum * self.running_var + (1-self.momentum) * var            \n",
    "        else:\n",
    "            xc = x - self.running_mean\n",
    "            xn = xc / ((np.sqrt(self.running_var + 10e-7)))\n",
    "            \n",
    "        out = self.gamma * xn + self.beta \n",
    "        return out\n",
    "\n",
    "    def backward(self, dout):\n",
    "        if dout.ndim != 2:\n",
    "            N, C, H, W = dout.shape\n",
    "            dout = dout.reshape(N, -1)\n",
    "\n",
    "        dx = self.__backward(dout)\n",
    "\n",
    "        dx = dx.reshape(*self.input_shape)\n",
    "        return dx\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PK-soITrQdTS"
   },
   "source": [
    "## 6. 多層パーセプトロンの実装<a class=\"anchor\" id=\"mlp\"></a>\n",
    "- 今までに実装してきた活性化関数，Linear層を組み合わせて，多層のパーセプトロンを実装する．\n",
    "- ここでは，先に実装したLinear層を組み合わせ，全体を通した**順伝播計算**と，**損失の計算**，**誤差逆伝播計算**，及び**重み・バイアスの更新**を実装する．\n",
    "- 例：3層・隠れ層のユニット数が1000．活性化関数はReLUを用いる場合\n",
    "```python\n",
    "model = MLP([Linear(784, 1000, ReLU),\n",
    "                        Linear(1000, 1000, ReLU),\n",
    "                        Linear(1000, 10, Softmax)])\n",
    "```\n",
    "- **lr** : 学習率 (learning rate)．学習率とは，重み・バイアスの更新量を決定するハイパーパラメータ．つまり，パラメータ更新量 = 学習率 × 現在の勾配"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bYqEi8TbHzLW"
   },
   "source": [
    "### 全体像の再掲\n",
    "<img src=\"https://drive.google.com/uc?export=download&id=11PBGMeAYpaeOtFH43FpEnrwZPbn8XbDf\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ucD2gz43QdTV"
   },
   "source": [
    "### <font color=\"crimson\">課題</font> : 多層パーセプトロン"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QJMFMxeiQdTW"
   },
   "outputs": [],
   "source": [
    "class MLP():\n",
    "    def __init__(self, layers):\n",
    "        self.layers = layers\n",
    "        \n",
    "    def train(self, x, t, lr, train_flg=True):     \n",
    "        # 1. 順伝播\n",
    "        self.y = x\n",
    "        for layer in self.layers:\n",
    "            self.y = layer(self.y, train_flg)  # 順伝播計算を順番に行い， 出力 y を計算しよう\n",
    "        \n",
    "        # 2. 損失関数の計算\n",
    "        self.loss = np.sum(-t*np.log(self.y + 1e-7)) / len(x)\n",
    "        \n",
    "        # 3. 誤差逆伝播\n",
    "        # 3.1. 最終層\n",
    "        # 3.1.1. 最終層の誤差・勾配計算\n",
    "        batchsize = len(self.layers[-1].x)\n",
    "        delta = (self.y - t) / batchsize\n",
    "        self.layers[-1].delta = delta\n",
    "        self.layers[-1].dW = np.dot(self.layers[-1].x.T, self.layers[-1].delta)\n",
    "        self.layers[-1].db = np.dot(np.ones(batchsize), self.layers[-1].delta)\n",
    "        dout = np.dot(self.layers[-1].delta, self.layers[-1].W.T)\n",
    "        \n",
    "        # 3.1.2. 最終層のパラメータ更新\n",
    "        self.layers[-1].W -= lr * self.layers[-1].dW # self.layers[-1].dW を用いて最終層の重みを更新しよう\n",
    "        self.layers[-1].b -= lr * self.layers[-1].db  # self.layers[-1].db を用いて最終層のバイアスを更新しよう\n",
    "        \n",
    "        # 3.2. 中間層\n",
    "        for layer in self.layers[-2::-1]:\n",
    "            # 3.2.1. 中間層の誤差・勾配計算\n",
    "            dout = layer.backward(dout)  # 逆伝播計算を順番に実行しよう\n",
    "            \n",
    "            # 3.2.2. パラメータの更新\n",
    "            layer.W -= lr * layer.dW  # 各層の重みを更新\n",
    "            layer.b -= lr *layer.db # 各層のバイアスを更新\n",
    "            \n",
    "        return self.loss\n",
    "\n",
    "    def test(self, x, t, train_flg=False):\n",
    "        # 性能をテストデータで調べるために用いる\n",
    "        # よって，誤差逆伝播は不要\n",
    "        # 順伝播 (train関数と同様)\n",
    "        self.y = x\n",
    "        for layer in self.layers:\n",
    "            self.y = layer(self.y, train_flg)\n",
    "        self.loss = np.sum(-t*np.log(self.y + 1e-7)) / len(x)\n",
    "        return self.loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LcpDSNp5QdTY"
   },
   "source": [
    "## モデルの構築\n",
    "- ここでは，図に示してきたような3層のニューラルネットワークを構築する\n",
    "- 活性化関数はSigmoid関数とし， 隠れ層のニューロン数はいずれも1000とする．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NhRhi9LlQdTd"
   },
   "source": [
    "# 7. 学習<a class=\"anchor\" id=\"train\"></a>\n",
    "\n",
    "- n_epoch : エポック数．1エポックとは，学習時に訓練データをすべて学習した回数を表す．\n",
    "- batchsize: バッチサイズ．\n",
    "- lr: 学習率 (learning rate)．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 67
    },
    "colab_type": "code",
    "id": "lMfyQGcGQdTd",
    "outputId": "f5f6de09-27ec-4afa-9be1-605a16a9a971",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_epoch=300, batchsize=100, lr=0.01\n",
      "epoch 0 | Train loss 2.272, accuracy 0.1531 | Test loss 2.089, accuracy 0.5879\n",
      "epoch 1 | Train loss 1.924, accuracy 0.3443 | Test loss 1.224, accuracy 0.6434\n",
      "epoch 2 | Train loss 1.197, accuracy 0.5757 | Test loss 0.696, accuracy 0.7644\n",
      "epoch 3 | Train loss 0.852, accuracy 0.7055 | Test loss 0.521, accuracy 0.8329\n",
      "epoch 4 | Train loss 0.676, accuracy 0.7773 | Test loss 0.423, accuracy 0.8677\n",
      "epoch 5 | Train loss 0.567, accuracy 0.8170 | Test loss 0.359, accuracy 0.8896\n",
      "epoch 6 | Train loss 0.498, accuracy 0.8426 | Test loss 0.319, accuracy 0.9043\n",
      "epoch 7 | Train loss 0.444, accuracy 0.8633 | Test loss 0.292, accuracy 0.9129\n",
      "epoch 8 | Train loss 0.408, accuracy 0.8762 | Test loss 0.271, accuracy 0.9181\n",
      "epoch 9 | Train loss 0.376, accuracy 0.8851 | Test loss 0.253, accuracy 0.9243\n",
      "epoch 10 | Train loss 0.350, accuracy 0.8938 | Test loss 0.234, accuracy 0.9301\n",
      "epoch 11 | Train loss 0.327, accuracy 0.9026 | Test loss 0.218, accuracy 0.9345\n",
      "epoch 12 | Train loss 0.311, accuracy 0.9063 | Test loss 0.209, accuracy 0.9371\n",
      "epoch 13 | Train loss 0.292, accuracy 0.9116 | Test loss 0.196, accuracy 0.9417\n",
      "epoch 14 | Train loss 0.279, accuracy 0.9161 | Test loss 0.186, accuracy 0.9443\n",
      "epoch 15 | Train loss 0.265, accuracy 0.9214 | Test loss 0.177, accuracy 0.9471\n",
      "epoch 16 | Train loss 0.255, accuracy 0.9239 | Test loss 0.171, accuracy 0.9489\n",
      "epoch 17 | Train loss 0.241, accuracy 0.9282 | Test loss 0.166, accuracy 0.9506\n",
      "epoch 18 | Train loss 0.232, accuracy 0.9301 | Test loss 0.159, accuracy 0.9528\n",
      "epoch 19 | Train loss 0.226, accuracy 0.9331 | Test loss 0.153, accuracy 0.9549\n",
      "epoch 20 | Train loss 0.216, accuracy 0.9360 | Test loss 0.146, accuracy 0.9564\n",
      "epoch 21 | Train loss 0.206, accuracy 0.9386 | Test loss 0.142, accuracy 0.9579\n",
      "epoch 22 | Train loss 0.201, accuracy 0.9414 | Test loss 0.138, accuracy 0.9589\n",
      "epoch 23 | Train loss 0.194, accuracy 0.9416 | Test loss 0.134, accuracy 0.9601\n",
      "epoch 24 | Train loss 0.186, accuracy 0.9450 | Test loss 0.131, accuracy 0.9609\n",
      "epoch 25 | Train loss 0.180, accuracy 0.9471 | Test loss 0.128, accuracy 0.9614\n",
      "epoch 26 | Train loss 0.173, accuracy 0.9474 | Test loss 0.125, accuracy 0.9626\n",
      "epoch 27 | Train loss 0.169, accuracy 0.9487 | Test loss 0.122, accuracy 0.9636\n",
      "epoch 28 | Train loss 0.164, accuracy 0.9514 | Test loss 0.118, accuracy 0.9645\n",
      "epoch 29 | Train loss 0.159, accuracy 0.9529 | Test loss 0.116, accuracy 0.9651\n",
      "epoch 30 | Train loss 0.157, accuracy 0.9537 | Test loss 0.114, accuracy 0.9657\n",
      "epoch 31 | Train loss 0.150, accuracy 0.9553 | Test loss 0.112, accuracy 0.9664\n",
      "epoch 32 | Train loss 0.148, accuracy 0.9563 | Test loss 0.109, accuracy 0.9674\n",
      "epoch 33 | Train loss 0.146, accuracy 0.9573 | Test loss 0.107, accuracy 0.9676\n",
      "epoch 34 | Train loss 0.143, accuracy 0.9577 | Test loss 0.104, accuracy 0.9687\n",
      "epoch 35 | Train loss 0.138, accuracy 0.9593 | Test loss 0.102, accuracy 0.9691\n",
      "epoch 36 | Train loss 0.134, accuracy 0.9590 | Test loss 0.101, accuracy 0.9695\n",
      "epoch 37 | Train loss 0.131, accuracy 0.9605 | Test loss 0.100, accuracy 0.9703\n",
      "epoch 38 | Train loss 0.128, accuracy 0.9618 | Test loss 0.098, accuracy 0.9706\n",
      "epoch 39 | Train loss 0.124, accuracy 0.9628 | Test loss 0.097, accuracy 0.9710\n",
      "epoch 40 | Train loss 0.123, accuracy 0.9633 | Test loss 0.095, accuracy 0.9717\n",
      "epoch 41 | Train loss 0.117, accuracy 0.9652 | Test loss 0.093, accuracy 0.9724\n",
      "epoch 42 | Train loss 0.116, accuracy 0.9659 | Test loss 0.091, accuracy 0.9725\n",
      "epoch 43 | Train loss 0.111, accuracy 0.9666 | Test loss 0.090, accuracy 0.9725\n",
      "epoch 44 | Train loss 0.112, accuracy 0.9669 | Test loss 0.089, accuracy 0.9733\n",
      "epoch 45 | Train loss 0.110, accuracy 0.9676 | Test loss 0.089, accuracy 0.9733\n",
      "epoch 46 | Train loss 0.106, accuracy 0.9681 | Test loss 0.088, accuracy 0.9742\n",
      "epoch 47 | Train loss 0.101, accuracy 0.9695 | Test loss 0.086, accuracy 0.9745\n",
      "epoch 48 | Train loss 0.103, accuracy 0.9695 | Test loss 0.085, accuracy 0.9746\n",
      "epoch 49 | Train loss 0.098, accuracy 0.9709 | Test loss 0.085, accuracy 0.9746\n",
      "epoch 50 | Train loss 0.097, accuracy 0.9706 | Test loss 0.084, accuracy 0.9749\n",
      "epoch 51 | Train loss 0.096, accuracy 0.9714 | Test loss 0.083, accuracy 0.9754\n",
      "epoch 52 | Train loss 0.092, accuracy 0.9727 | Test loss 0.082, accuracy 0.9754\n",
      "epoch 53 | Train loss 0.093, accuracy 0.9714 | Test loss 0.082, accuracy 0.9751\n",
      "epoch 54 | Train loss 0.090, accuracy 0.9733 | Test loss 0.080, accuracy 0.9756\n",
      "epoch 55 | Train loss 0.088, accuracy 0.9729 | Test loss 0.080, accuracy 0.9756\n",
      "epoch 56 | Train loss 0.086, accuracy 0.9739 | Test loss 0.080, accuracy 0.9758\n",
      "epoch 57 | Train loss 0.085, accuracy 0.9739 | Test loss 0.079, accuracy 0.9756\n",
      "epoch 58 | Train loss 0.082, accuracy 0.9754 | Test loss 0.078, accuracy 0.9757\n",
      "epoch 59 | Train loss 0.081, accuracy 0.9751 | Test loss 0.077, accuracy 0.9766\n",
      "epoch 60 | Train loss 0.081, accuracy 0.9754 | Test loss 0.077, accuracy 0.9766\n",
      "epoch 61 | Train loss 0.079, accuracy 0.9755 | Test loss 0.076, accuracy 0.9771\n",
      "epoch 62 | Train loss 0.075, accuracy 0.9768 | Test loss 0.075, accuracy 0.9768\n",
      "epoch 63 | Train loss 0.075, accuracy 0.9768 | Test loss 0.075, accuracy 0.9771\n",
      "epoch 64 | Train loss 0.072, accuracy 0.9774 | Test loss 0.075, accuracy 0.9778\n",
      "epoch 65 | Train loss 0.073, accuracy 0.9778 | Test loss 0.074, accuracy 0.9771\n",
      "epoch 66 | Train loss 0.072, accuracy 0.9778 | Test loss 0.074, accuracy 0.9779\n",
      "epoch 67 | Train loss 0.068, accuracy 0.9785 | Test loss 0.073, accuracy 0.9781\n",
      "epoch 68 | Train loss 0.067, accuracy 0.9794 | Test loss 0.073, accuracy 0.9779\n",
      "epoch 69 | Train loss 0.069, accuracy 0.9785 | Test loss 0.072, accuracy 0.9782\n",
      "epoch 70 | Train loss 0.065, accuracy 0.9803 | Test loss 0.072, accuracy 0.9781\n",
      "epoch 71 | Train loss 0.065, accuracy 0.9800 | Test loss 0.072, accuracy 0.9782\n",
      "epoch 72 | Train loss 0.062, accuracy 0.9808 | Test loss 0.072, accuracy 0.9789\n",
      "epoch 73 | Train loss 0.064, accuracy 0.9805 | Test loss 0.071, accuracy 0.9784\n",
      "epoch 74 | Train loss 0.061, accuracy 0.9811 | Test loss 0.071, accuracy 0.9786\n",
      "epoch 75 | Train loss 0.059, accuracy 0.9814 | Test loss 0.070, accuracy 0.9789\n",
      "epoch 76 | Train loss 0.056, accuracy 0.9827 | Test loss 0.071, accuracy 0.9790\n",
      "epoch 77 | Train loss 0.057, accuracy 0.9824 | Test loss 0.071, accuracy 0.9796\n",
      "epoch 78 | Train loss 0.056, accuracy 0.9827 | Test loss 0.070, accuracy 0.9791\n",
      "epoch 79 | Train loss 0.056, accuracy 0.9829 | Test loss 0.071, accuracy 0.9791\n",
      "epoch 80 | Train loss 0.054, accuracy 0.9831 | Test loss 0.069, accuracy 0.9793\n",
      "epoch 81 | Train loss 0.053, accuracy 0.9834 | Test loss 0.069, accuracy 0.9801\n",
      "epoch 82 | Train loss 0.052, accuracy 0.9845 | Test loss 0.069, accuracy 0.9799\n",
      "epoch 83 | Train loss 0.052, accuracy 0.9837 | Test loss 0.069, accuracy 0.9804\n",
      "epoch 84 | Train loss 0.052, accuracy 0.9839 | Test loss 0.070, accuracy 0.9794\n",
      "epoch 85 | Train loss 0.050, accuracy 0.9839 | Test loss 0.069, accuracy 0.9795\n",
      "epoch 86 | Train loss 0.048, accuracy 0.9853 | Test loss 0.069, accuracy 0.9794\n",
      "epoch 87 | Train loss 0.049, accuracy 0.9847 | Test loss 0.069, accuracy 0.9802\n",
      "epoch 88 | Train loss 0.047, accuracy 0.9854 | Test loss 0.068, accuracy 0.9808\n",
      "epoch 89 | Train loss 0.046, accuracy 0.9857 | Test loss 0.069, accuracy 0.9802\n",
      "epoch 90 | Train loss 0.047, accuracy 0.9851 | Test loss 0.068, accuracy 0.9804\n",
      "epoch 91 | Train loss 0.046, accuracy 0.9854 | Test loss 0.068, accuracy 0.9803\n",
      "epoch 92 | Train loss 0.043, accuracy 0.9866 | Test loss 0.068, accuracy 0.9809\n",
      "epoch 93 | Train loss 0.045, accuracy 0.9862 | Test loss 0.068, accuracy 0.9809\n",
      "epoch 94 | Train loss 0.041, accuracy 0.9872 | Test loss 0.068, accuracy 0.9807\n",
      "epoch 95 | Train loss 0.042, accuracy 0.9865 | Test loss 0.068, accuracy 0.9809\n",
      "epoch 96 | Train loss 0.041, accuracy 0.9867 | Test loss 0.067, accuracy 0.9809\n",
      "epoch 97 | Train loss 0.041, accuracy 0.9868 | Test loss 0.068, accuracy 0.9809\n",
      "epoch 98 | Train loss 0.039, accuracy 0.9879 | Test loss 0.068, accuracy 0.9809\n",
      "epoch 99 | Train loss 0.040, accuracy 0.9875 | Test loss 0.068, accuracy 0.9808\n",
      "epoch 100 | Train loss 0.039, accuracy 0.9877 | Test loss 0.068, accuracy 0.9807\n",
      "epoch 101 | Train loss 0.039, accuracy 0.9883 | Test loss 0.068, accuracy 0.9811\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 102 | Train loss 0.037, accuracy 0.9883 | Test loss 0.068, accuracy 0.9814\n",
      "epoch 103 | Train loss 0.036, accuracy 0.9887 | Test loss 0.068, accuracy 0.9814\n",
      "epoch 104 | Train loss 0.035, accuracy 0.9887 | Test loss 0.067, accuracy 0.9817\n",
      "epoch 105 | Train loss 0.036, accuracy 0.9892 | Test loss 0.067, accuracy 0.9813\n",
      "epoch 106 | Train loss 0.033, accuracy 0.9889 | Test loss 0.068, accuracy 0.9812\n",
      "epoch 107 | Train loss 0.034, accuracy 0.9892 | Test loss 0.068, accuracy 0.9816\n",
      "epoch 108 | Train loss 0.035, accuracy 0.9891 | Test loss 0.067, accuracy 0.9821\n",
      "epoch 109 | Train loss 0.033, accuracy 0.9892 | Test loss 0.067, accuracy 0.9821\n",
      "epoch 110 | Train loss 0.032, accuracy 0.9897 | Test loss 0.067, accuracy 0.9821\n",
      "epoch 111 | Train loss 0.032, accuracy 0.9894 | Test loss 0.067, accuracy 0.9821\n",
      "epoch 112 | Train loss 0.031, accuracy 0.9903 | Test loss 0.067, accuracy 0.9819\n",
      "epoch 113 | Train loss 0.030, accuracy 0.9909 | Test loss 0.068, accuracy 0.9816\n",
      "epoch 114 | Train loss 0.031, accuracy 0.9900 | Test loss 0.067, accuracy 0.9820\n",
      "epoch 115 | Train loss 0.029, accuracy 0.9908 | Test loss 0.068, accuracy 0.9819\n",
      "epoch 116 | Train loss 0.031, accuracy 0.9904 | Test loss 0.067, accuracy 0.9820\n",
      "epoch 117 | Train loss 0.029, accuracy 0.9912 | Test loss 0.067, accuracy 0.9819\n",
      "epoch 118 | Train loss 0.028, accuracy 0.9912 | Test loss 0.068, accuracy 0.9819\n",
      "epoch 119 | Train loss 0.029, accuracy 0.9904 | Test loss 0.068, accuracy 0.9818\n",
      "epoch 120 | Train loss 0.028, accuracy 0.9908 | Test loss 0.069, accuracy 0.9824\n",
      "epoch 121 | Train loss 0.028, accuracy 0.9913 | Test loss 0.068, accuracy 0.9822\n",
      "epoch 122 | Train loss 0.026, accuracy 0.9917 | Test loss 0.069, accuracy 0.9819\n",
      "epoch 123 | Train loss 0.028, accuracy 0.9915 | Test loss 0.068, accuracy 0.9824\n",
      "epoch 124 | Train loss 0.026, accuracy 0.9915 | Test loss 0.068, accuracy 0.9819\n",
      "epoch 125 | Train loss 0.025, accuracy 0.9921 | Test loss 0.069, accuracy 0.9819\n",
      "epoch 126 | Train loss 0.026, accuracy 0.9917 | Test loss 0.069, accuracy 0.9816\n",
      "epoch 127 | Train loss 0.025, accuracy 0.9922 | Test loss 0.069, accuracy 0.9821\n",
      "epoch 128 | Train loss 0.025, accuracy 0.9925 | Test loss 0.070, accuracy 0.9822\n",
      "epoch 129 | Train loss 0.025, accuracy 0.9921 | Test loss 0.070, accuracy 0.9818\n",
      "epoch 130 | Train loss 0.023, accuracy 0.9929 | Test loss 0.069, accuracy 0.9821\n",
      "epoch 131 | Train loss 0.024, accuracy 0.9925 | Test loss 0.069, accuracy 0.9817\n",
      "epoch 132 | Train loss 0.023, accuracy 0.9928 | Test loss 0.070, accuracy 0.9821\n",
      "epoch 133 | Train loss 0.022, accuracy 0.9931 | Test loss 0.070, accuracy 0.9825\n",
      "epoch 134 | Train loss 0.023, accuracy 0.9925 | Test loss 0.069, accuracy 0.9824\n",
      "epoch 135 | Train loss 0.022, accuracy 0.9931 | Test loss 0.068, accuracy 0.9828\n",
      "epoch 136 | Train loss 0.022, accuracy 0.9932 | Test loss 0.069, accuracy 0.9826\n",
      "epoch 137 | Train loss 0.022, accuracy 0.9931 | Test loss 0.069, accuracy 0.9828\n",
      "epoch 138 | Train loss 0.021, accuracy 0.9935 | Test loss 0.069, accuracy 0.9827\n",
      "epoch 139 | Train loss 0.022, accuracy 0.9931 | Test loss 0.070, accuracy 0.9821\n",
      "epoch 140 | Train loss 0.022, accuracy 0.9930 | Test loss 0.069, accuracy 0.9827\n",
      "epoch 141 | Train loss 0.020, accuracy 0.9938 | Test loss 0.070, accuracy 0.9823\n",
      "epoch 142 | Train loss 0.020, accuracy 0.9940 | Test loss 0.071, accuracy 0.9825\n",
      "epoch 143 | Train loss 0.021, accuracy 0.9936 | Test loss 0.070, accuracy 0.9824\n",
      "epoch 144 | Train loss 0.019, accuracy 0.9941 | Test loss 0.070, accuracy 0.9826\n",
      "epoch 145 | Train loss 0.019, accuracy 0.9941 | Test loss 0.070, accuracy 0.9826\n",
      "epoch 146 | Train loss 0.020, accuracy 0.9939 | Test loss 0.070, accuracy 0.9823\n",
      "epoch 147 | Train loss 0.020, accuracy 0.9936 | Test loss 0.070, accuracy 0.9825\n",
      "epoch 148 | Train loss 0.019, accuracy 0.9937 | Test loss 0.069, accuracy 0.9829\n",
      "epoch 149 | Train loss 0.020, accuracy 0.9937 | Test loss 0.069, accuracy 0.9829\n",
      "epoch 150 | Train loss 0.018, accuracy 0.9947 | Test loss 0.070, accuracy 0.9827\n",
      "epoch 151 | Train loss 0.018, accuracy 0.9944 | Test loss 0.071, accuracy 0.9823\n",
      "epoch 152 | Train loss 0.018, accuracy 0.9944 | Test loss 0.071, accuracy 0.9829\n",
      "epoch 153 | Train loss 0.017, accuracy 0.9942 | Test loss 0.070, accuracy 0.9828\n",
      "epoch 154 | Train loss 0.017, accuracy 0.9946 | Test loss 0.070, accuracy 0.9829\n",
      "epoch 155 | Train loss 0.017, accuracy 0.9946 | Test loss 0.070, accuracy 0.9831\n",
      "epoch 156 | Train loss 0.017, accuracy 0.9946 | Test loss 0.071, accuracy 0.9829\n",
      "epoch 157 | Train loss 0.016, accuracy 0.9952 | Test loss 0.071, accuracy 0.9831\n",
      "epoch 158 | Train loss 0.017, accuracy 0.9948 | Test loss 0.071, accuracy 0.9829\n",
      "epoch 159 | Train loss 0.016, accuracy 0.9948 | Test loss 0.071, accuracy 0.9828\n",
      "epoch 160 | Train loss 0.017, accuracy 0.9945 | Test loss 0.071, accuracy 0.9829\n",
      "epoch 161 | Train loss 0.016, accuracy 0.9950 | Test loss 0.070, accuracy 0.9831\n",
      "epoch 162 | Train loss 0.017, accuracy 0.9945 | Test loss 0.070, accuracy 0.9830\n",
      "epoch 163 | Train loss 0.016, accuracy 0.9952 | Test loss 0.072, accuracy 0.9824\n",
      "epoch 164 | Train loss 0.017, accuracy 0.9948 | Test loss 0.071, accuracy 0.9829\n",
      "epoch 165 | Train loss 0.015, accuracy 0.9950 | Test loss 0.070, accuracy 0.9831\n",
      "epoch 166 | Train loss 0.016, accuracy 0.9949 | Test loss 0.072, accuracy 0.9825\n",
      "epoch 167 | Train loss 0.016, accuracy 0.9950 | Test loss 0.072, accuracy 0.9826\n",
      "epoch 168 | Train loss 0.016, accuracy 0.9947 | Test loss 0.072, accuracy 0.9828\n",
      "epoch 169 | Train loss 0.014, accuracy 0.9951 | Test loss 0.070, accuracy 0.9829\n",
      "epoch 170 | Train loss 0.015, accuracy 0.9956 | Test loss 0.072, accuracy 0.9829\n",
      "epoch 171 | Train loss 0.014, accuracy 0.9957 | Test loss 0.072, accuracy 0.9825\n",
      "epoch 172 | Train loss 0.014, accuracy 0.9957 | Test loss 0.072, accuracy 0.9836\n",
      "epoch 173 | Train loss 0.015, accuracy 0.9955 | Test loss 0.071, accuracy 0.9831\n",
      "epoch 174 | Train loss 0.014, accuracy 0.9956 | Test loss 0.072, accuracy 0.9830\n",
      "epoch 175 | Train loss 0.015, accuracy 0.9953 | Test loss 0.071, accuracy 0.9834\n",
      "epoch 176 | Train loss 0.013, accuracy 0.9957 | Test loss 0.072, accuracy 0.9832\n",
      "epoch 177 | Train loss 0.013, accuracy 0.9957 | Test loss 0.073, accuracy 0.9829\n",
      "epoch 178 | Train loss 0.014, accuracy 0.9955 | Test loss 0.072, accuracy 0.9833\n",
      "epoch 179 | Train loss 0.013, accuracy 0.9959 | Test loss 0.073, accuracy 0.9834\n",
      "epoch 180 | Train loss 0.013, accuracy 0.9955 | Test loss 0.073, accuracy 0.9831\n",
      "epoch 181 | Train loss 0.014, accuracy 0.9956 | Test loss 0.072, accuracy 0.9836\n",
      "epoch 182 | Train loss 0.014, accuracy 0.9956 | Test loss 0.073, accuracy 0.9835\n",
      "epoch 183 | Train loss 0.012, accuracy 0.9962 | Test loss 0.074, accuracy 0.9831\n",
      "epoch 184 | Train loss 0.012, accuracy 0.9958 | Test loss 0.073, accuracy 0.9834\n",
      "epoch 185 | Train loss 0.012, accuracy 0.9964 | Test loss 0.073, accuracy 0.9836\n",
      "epoch 186 | Train loss 0.011, accuracy 0.9966 | Test loss 0.073, accuracy 0.9836\n",
      "epoch 187 | Train loss 0.012, accuracy 0.9961 | Test loss 0.073, accuracy 0.9836\n",
      "epoch 188 | Train loss 0.012, accuracy 0.9961 | Test loss 0.073, accuracy 0.9834\n",
      "epoch 189 | Train loss 0.012, accuracy 0.9962 | Test loss 0.074, accuracy 0.9834\n",
      "epoch 190 | Train loss 0.012, accuracy 0.9961 | Test loss 0.072, accuracy 0.9832\n",
      "epoch 191 | Train loss 0.012, accuracy 0.9962 | Test loss 0.073, accuracy 0.9830\n",
      "epoch 192 | Train loss 0.011, accuracy 0.9964 | Test loss 0.073, accuracy 0.9835\n",
      "epoch 193 | Train loss 0.011, accuracy 0.9966 | Test loss 0.074, accuracy 0.9836\n",
      "epoch 194 | Train loss 0.011, accuracy 0.9964 | Test loss 0.073, accuracy 0.9831\n",
      "epoch 195 | Train loss 0.011, accuracy 0.9967 | Test loss 0.073, accuracy 0.9834\n",
      "epoch 196 | Train loss 0.011, accuracy 0.9962 | Test loss 0.072, accuracy 0.9837\n",
      "epoch 197 | Train loss 0.011, accuracy 0.9964 | Test loss 0.073, accuracy 0.9833\n",
      "epoch 198 | Train loss 0.011, accuracy 0.9964 | Test loss 0.073, accuracy 0.9834\n",
      "epoch 199 | Train loss 0.011, accuracy 0.9965 | Test loss 0.073, accuracy 0.9832\n",
      "epoch 200 | Train loss 0.010, accuracy 0.9965 | Test loss 0.074, accuracy 0.9834\n",
      "epoch 201 | Train loss 0.011, accuracy 0.9965 | Test loss 0.074, accuracy 0.9831\n",
      "epoch 202 | Train loss 0.010, accuracy 0.9968 | Test loss 0.075, accuracy 0.9835\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 203 | Train loss 0.010, accuracy 0.9972 | Test loss 0.074, accuracy 0.9838\n",
      "epoch 204 | Train loss 0.010, accuracy 0.9965 | Test loss 0.077, accuracy 0.9834\n",
      "epoch 205 | Train loss 0.011, accuracy 0.9969 | Test loss 0.075, accuracy 0.9832\n",
      "epoch 206 | Train loss 0.010, accuracy 0.9966 | Test loss 0.074, accuracy 0.9834\n",
      "epoch 207 | Train loss 0.009, accuracy 0.9969 | Test loss 0.075, accuracy 0.9837\n",
      "epoch 208 | Train loss 0.010, accuracy 0.9972 | Test loss 0.075, accuracy 0.9834\n",
      "epoch 209 | Train loss 0.010, accuracy 0.9969 | Test loss 0.075, accuracy 0.9832\n",
      "epoch 210 | Train loss 0.009, accuracy 0.9974 | Test loss 0.075, accuracy 0.9835\n",
      "epoch 211 | Train loss 0.010, accuracy 0.9968 | Test loss 0.075, accuracy 0.9835\n",
      "epoch 212 | Train loss 0.009, accuracy 0.9975 | Test loss 0.077, accuracy 0.9834\n",
      "epoch 213 | Train loss 0.010, accuracy 0.9972 | Test loss 0.076, accuracy 0.9834\n",
      "epoch 214 | Train loss 0.008, accuracy 0.9975 | Test loss 0.077, accuracy 0.9832\n",
      "epoch 215 | Train loss 0.009, accuracy 0.9972 | Test loss 0.076, accuracy 0.9840\n",
      "epoch 216 | Train loss 0.010, accuracy 0.9970 | Test loss 0.075, accuracy 0.9833\n",
      "epoch 217 | Train loss 0.009, accuracy 0.9973 | Test loss 0.077, accuracy 0.9834\n",
      "epoch 218 | Train loss 0.009, accuracy 0.9973 | Test loss 0.076, accuracy 0.9829\n",
      "epoch 219 | Train loss 0.009, accuracy 0.9972 | Test loss 0.076, accuracy 0.9834\n",
      "epoch 220 | Train loss 0.009, accuracy 0.9972 | Test loss 0.075, accuracy 0.9835\n",
      "epoch 221 | Train loss 0.009, accuracy 0.9973 | Test loss 0.075, accuracy 0.9836\n",
      "epoch 222 | Train loss 0.008, accuracy 0.9975 | Test loss 0.076, accuracy 0.9829\n",
      "epoch 223 | Train loss 0.008, accuracy 0.9975 | Test loss 0.076, accuracy 0.9831\n",
      "epoch 224 | Train loss 0.009, accuracy 0.9974 | Test loss 0.077, accuracy 0.9831\n",
      "epoch 225 | Train loss 0.008, accuracy 0.9974 | Test loss 0.076, accuracy 0.9838\n",
      "epoch 226 | Train loss 0.008, accuracy 0.9976 | Test loss 0.077, accuracy 0.9834\n",
      "epoch 227 | Train loss 0.008, accuracy 0.9975 | Test loss 0.076, accuracy 0.9836\n",
      "epoch 228 | Train loss 0.009, accuracy 0.9973 | Test loss 0.075, accuracy 0.9837\n",
      "epoch 229 | Train loss 0.008, accuracy 0.9974 | Test loss 0.075, accuracy 0.9840\n",
      "epoch 230 | Train loss 0.008, accuracy 0.9974 | Test loss 0.077, accuracy 0.9837\n",
      "epoch 231 | Train loss 0.008, accuracy 0.9975 | Test loss 0.077, accuracy 0.9840\n",
      "epoch 232 | Train loss 0.008, accuracy 0.9974 | Test loss 0.077, accuracy 0.9841\n",
      "epoch 233 | Train loss 0.007, accuracy 0.9976 | Test loss 0.077, accuracy 0.9837\n",
      "epoch 234 | Train loss 0.008, accuracy 0.9972 | Test loss 0.077, accuracy 0.9834\n",
      "epoch 235 | Train loss 0.008, accuracy 0.9974 | Test loss 0.077, accuracy 0.9838\n",
      "epoch 236 | Train loss 0.008, accuracy 0.9976 | Test loss 0.075, accuracy 0.9838\n",
      "epoch 237 | Train loss 0.007, accuracy 0.9978 | Test loss 0.077, accuracy 0.9835\n",
      "epoch 238 | Train loss 0.007, accuracy 0.9976 | Test loss 0.076, accuracy 0.9838\n",
      "epoch 239 | Train loss 0.008, accuracy 0.9973 | Test loss 0.076, accuracy 0.9839\n",
      "epoch 240 | Train loss 0.008, accuracy 0.9975 | Test loss 0.078, accuracy 0.9841\n",
      "epoch 241 | Train loss 0.008, accuracy 0.9978 | Test loss 0.077, accuracy 0.9836\n",
      "epoch 242 | Train loss 0.008, accuracy 0.9976 | Test loss 0.078, accuracy 0.9839\n",
      "epoch 243 | Train loss 0.007, accuracy 0.9976 | Test loss 0.079, accuracy 0.9838\n",
      "epoch 244 | Train loss 0.007, accuracy 0.9978 | Test loss 0.077, accuracy 0.9841\n",
      "epoch 245 | Train loss 0.007, accuracy 0.9978 | Test loss 0.077, accuracy 0.9839\n",
      "epoch 246 | Train loss 0.008, accuracy 0.9975 | Test loss 0.077, accuracy 0.9835\n",
      "epoch 247 | Train loss 0.007, accuracy 0.9979 | Test loss 0.077, accuracy 0.9839\n",
      "epoch 248 | Train loss 0.007, accuracy 0.9976 | Test loss 0.079, accuracy 0.9835\n",
      "epoch 249 | Train loss 0.007, accuracy 0.9981 | Test loss 0.080, accuracy 0.9839\n",
      "epoch 250 | Train loss 0.007, accuracy 0.9979 | Test loss 0.079, accuracy 0.9836\n",
      "epoch 251 | Train loss 0.007, accuracy 0.9977 | Test loss 0.079, accuracy 0.9834\n",
      "epoch 252 | Train loss 0.007, accuracy 0.9979 | Test loss 0.079, accuracy 0.9834\n",
      "epoch 253 | Train loss 0.007, accuracy 0.9979 | Test loss 0.080, accuracy 0.9837\n",
      "epoch 254 | Train loss 0.007, accuracy 0.9978 | Test loss 0.080, accuracy 0.9835\n",
      "epoch 255 | Train loss 0.007, accuracy 0.9979 | Test loss 0.079, accuracy 0.9845\n",
      "epoch 256 | Train loss 0.006, accuracy 0.9980 | Test loss 0.078, accuracy 0.9841\n",
      "epoch 257 | Train loss 0.007, accuracy 0.9979 | Test loss 0.077, accuracy 0.9842\n",
      "epoch 258 | Train loss 0.007, accuracy 0.9979 | Test loss 0.077, accuracy 0.9839\n",
      "epoch 259 | Train loss 0.006, accuracy 0.9980 | Test loss 0.078, accuracy 0.9843\n",
      "epoch 260 | Train loss 0.006, accuracy 0.9981 | Test loss 0.079, accuracy 0.9836\n",
      "epoch 261 | Train loss 0.007, accuracy 0.9980 | Test loss 0.079, accuracy 0.9839\n",
      "epoch 262 | Train loss 0.006, accuracy 0.9977 | Test loss 0.078, accuracy 0.9835\n",
      "epoch 263 | Train loss 0.006, accuracy 0.9981 | Test loss 0.079, accuracy 0.9836\n",
      "epoch 264 | Train loss 0.006, accuracy 0.9981 | Test loss 0.079, accuracy 0.9838\n",
      "epoch 265 | Train loss 0.006, accuracy 0.9981 | Test loss 0.078, accuracy 0.9841\n",
      "epoch 266 | Train loss 0.005, accuracy 0.9985 | Test loss 0.080, accuracy 0.9839\n",
      "epoch 267 | Train loss 0.006, accuracy 0.9982 | Test loss 0.079, accuracy 0.9840\n",
      "epoch 268 | Train loss 0.006, accuracy 0.9982 | Test loss 0.080, accuracy 0.9837\n",
      "epoch 269 | Train loss 0.006, accuracy 0.9982 | Test loss 0.081, accuracy 0.9834\n",
      "epoch 270 | Train loss 0.006, accuracy 0.9982 | Test loss 0.081, accuracy 0.9844\n",
      "epoch 271 | Train loss 0.005, accuracy 0.9985 | Test loss 0.080, accuracy 0.9841\n",
      "epoch 272 | Train loss 0.006, accuracy 0.9983 | Test loss 0.081, accuracy 0.9844\n",
      "epoch 273 | Train loss 0.006, accuracy 0.9980 | Test loss 0.079, accuracy 0.9839\n",
      "epoch 274 | Train loss 0.007, accuracy 0.9977 | Test loss 0.080, accuracy 0.9842\n",
      "epoch 275 | Train loss 0.006, accuracy 0.9980 | Test loss 0.080, accuracy 0.9841\n",
      "epoch 276 | Train loss 0.006, accuracy 0.9981 | Test loss 0.080, accuracy 0.9839\n",
      "epoch 277 | Train loss 0.007, accuracy 0.9980 | Test loss 0.080, accuracy 0.9841\n",
      "epoch 278 | Train loss 0.006, accuracy 0.9982 | Test loss 0.080, accuracy 0.9845\n",
      "epoch 279 | Train loss 0.005, accuracy 0.9983 | Test loss 0.079, accuracy 0.9846\n",
      "epoch 280 | Train loss 0.006, accuracy 0.9982 | Test loss 0.080, accuracy 0.9841\n",
      "epoch 281 | Train loss 0.006, accuracy 0.9980 | Test loss 0.081, accuracy 0.9844\n",
      "epoch 282 | Train loss 0.006, accuracy 0.9982 | Test loss 0.080, accuracy 0.9839\n",
      "epoch 283 | Train loss 0.006, accuracy 0.9983 | Test loss 0.080, accuracy 0.9840\n",
      "epoch 284 | Train loss 0.004, accuracy 0.9988 | Test loss 0.079, accuracy 0.9846\n",
      "epoch 285 | Train loss 0.006, accuracy 0.9983 | Test loss 0.080, accuracy 0.9845\n",
      "epoch 286 | Train loss 0.006, accuracy 0.9982 | Test loss 0.081, accuracy 0.9835\n",
      "epoch 287 | Train loss 0.005, accuracy 0.9983 | Test loss 0.079, accuracy 0.9838\n",
      "epoch 288 | Train loss 0.005, accuracy 0.9981 | Test loss 0.082, accuracy 0.9839\n",
      "epoch 289 | Train loss 0.005, accuracy 0.9983 | Test loss 0.081, accuracy 0.9841\n",
      "epoch 290 | Train loss 0.006, accuracy 0.9986 | Test loss 0.080, accuracy 0.9839\n",
      "epoch 291 | Train loss 0.006, accuracy 0.9984 | Test loss 0.082, accuracy 0.9836\n",
      "epoch 292 | Train loss 0.005, accuracy 0.9985 | Test loss 0.082, accuracy 0.9835\n",
      "epoch 293 | Train loss 0.005, accuracy 0.9982 | Test loss 0.081, accuracy 0.9839\n",
      "epoch 294 | Train loss 0.005, accuracy 0.9984 | Test loss 0.081, accuracy 0.9841\n",
      "epoch 295 | Train loss 0.005, accuracy 0.9986 | Test loss 0.080, accuracy 0.9842\n",
      "epoch 296 | Train loss 0.005, accuracy 0.9984 | Test loss 0.080, accuracy 0.9844\n",
      "epoch 297 | Train loss 0.005, accuracy 0.9988 | Test loss 0.080, accuracy 0.9844\n",
      "epoch 298 | Train loss 0.005, accuracy 0.9984 | Test loss 0.081, accuracy 0.9842\n",
      "epoch 299 | Train loss 0.006, accuracy 0.9982 | Test loss 0.083, accuracy 0.9845\n"
     ]
    }
   ],
   "source": [
    "model = MLP([\n",
    "Linear(784, 1000, ReLU),\n",
    "Dropout(1000, 1000, 0.2),\n",
    "Linear(1000, 1000, ReLU),\n",
    "Dropout(1000, 1000, 0.5),\n",
    "Linear(1000, 1000, ReLU),\n",
    "Dropout(1000, 1000, 0.5),\n",
    "Linear(1000, 1000, ReLU),\n",
    "Dropout(1000, 1000, 0.5),\n",
    "Linear(1000, 1000, ReLU),\n",
    "Dropout(1000, 1000, 0.5),\n",
    "Linear(1000, 10, Softmax)])\n",
    "batchsize = 100\n",
    "for lr, n_epoch in [[0.01, 300]]:\n",
    "  print('n_epoch={}, batchsize={}, lr={}'.format(n_epoch,batchsize,lr))\n",
    "  for epoch in range(n_epoch):\n",
    "      print('epoch %d | ' % epoch, end=\"\")\n",
    "      \n",
    "      # 訓練\n",
    "      sum_loss = 0\n",
    "      pred_y = []\n",
    "      perm = np.random.permutation(train_n)\n",
    "      \n",
    "      for i in range(0, train_n, batchsize):\n",
    "          x = train_x[perm[i: i+batchsize]]\n",
    "          t = train_y[perm[i: i+batchsize]]\n",
    "          sum_loss += model.train(x, t, lr) * len(x)\n",
    "          # model.y には， (N, 10)の形で，画像が0~9の各数字のどれに分類されるかの事後確率が入っている\n",
    "          # そこで，最も大きい値をもつインデックスを取得することで，識別結果を得ることができる\n",
    "          pred_y.extend(np.argmax(model.y, axis=1))\n",
    "      \n",
    "      loss = sum_loss / train_n\n",
    "      \n",
    "      # accuracy : 予測結果を1-hot表現に変換し，正解との要素積の和を取ることで，正解数を計算できる．\n",
    "      accuracy = np.sum(np.eye(10)[pred_y] * train_y[perm]) / train_n\n",
    "      print('Train loss %.3f, accuracy %.4f | ' %(loss, accuracy), end=\"\")\n",
    "      \n",
    "      \n",
    "      # テスト\n",
    "      sum_loss = 0\n",
    "      pred_y = []\n",
    "      \n",
    "      for i in range(0, test_n, batchsize):\n",
    "          x = test_x[i: i+batchsize]\n",
    "          t = test_y[i: i+batchsize]\n",
    "          \n",
    "          sum_loss += model.test(x, t) * len(x)\n",
    "          pred_y.extend(np.argmax(model.y, axis=1))\n",
    "\n",
    "      loss = sum_loss / test_n\n",
    "      accuracy = np.sum(np.eye(10)[pred_y] * test_y) / test_n\n",
    "      print('Test loss %.3f, accuracy %.4f' %(loss, accuracy))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BW4C1UNhQdTf"
   },
   "source": [
    "初期設定では，98%前後のaccuracyになったのではないかと思います．\n",
    "\n",
    "ニューラルネットワークの学習には，様々なハイパーパラメータ（学習率など）を上手く設定する必要があります．\n",
    "\n",
    "また，活性化関数や，重みの初期値の工夫，Dropoutなどのテクニックを用いることで，さらに性能が向上する可能性があります．\n",
    "\n",
    "モデルを工夫して，よりよい性能を発揮するニューラルネットワークを構築してみましょう．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "j9wtLJpjIOmM"
   },
   "source": [
    "実行結果メモ\n",
    "(1)\n",
    "```\n",
    "model = MLP(\n",
    "[Linear(784, 1000, Sigmoid),\n",
    "Linear(1000, 1000, Sigmoid),\n",
    "Linear(1000, 10, Softmax)])\n",
    "n_epoch = 20*4\n",
    "batchsize = 100\n",
    "lr = 1,0.5,0.1,0.05,0.01\n",
    "```\n",
    "test accuracy: 0.9811\n",
    "\n",
    "(2)\n",
    "```\n",
    "model = MLP(\n",
    "[Linear(784, 1000, ReLU),\n",
    "Linear(1000, 1000, ReLU),\n",
    "Linear(1000, 10, Softmax)])\n",
    "n_epoch = 20\n",
    "batchsize = 100\n",
    "lr = 1,0.5,0.1,0.05,0.01\n",
    "```\n",
    "test accuracy: 0.9845\n",
    "\n",
    "DropoutとMish，せっかく実装したのにあんまり効果がなくて悲しい．\n",
    "もう少し層の深さが必要？\n",
    "\n",
    "(3)\n",
    "```\n",
    "model = MLP([\n",
    "Linear(784, 1000, ReLU),\n",
    "Linear(1000, 1000, ReLU),\n",
    "Linear(1000, 1000, ReLU),\n",
    "Dropout(1000, 1000, 0.5),\n",
    "Linear(1000, 10, Softmax)])\n",
    "n_epoch = 20\n",
    "batchsize = 100\n",
    "lr = 1,0.5,0.1,0.05,0.01\n",
    "epoch = 10,20,20,20,20\n",
    "初期値 = 一様分布\n",
    "```\n",
    "test accuracy: 0.9859\n",
    "\n",
    "過学習してますねぇ…\n",
    "\n",
    "```\n",
    "model = MLP([\n",
    "Linear(784, 1000, ReLU),\n",
    "Dropout(1000, 1000, 0.2),\n",
    "Linear(1000, 1000, ReLU),\n",
    "Dropout(1000, 1000, 0.5),\n",
    "Linear(1000, 1000, ReLU),\n",
    "Dropout(1000, 1000, 0.5),\n",
    "Linear(1000, 1000, ReLU),\n",
    "Dropout(1000, 1000, 0.5),\n",
    "Linear(1000, 1000, ReLU),\n",
    "Dropout(1000, 1000, 0.5),\n",
    "Linear(1000, 10, Softmax)])\n",
    "batchsize = 100\n",
    "lr, n_epoch in [ [0.1, 20], [0.05, 20], [0.01, 20], [0.005, 20]]\n",
    "```\n",
    "test accuracy: 0.9861\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "COz0Wud-rG4t"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "mlp_handson.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
